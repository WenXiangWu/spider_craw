# 智能网站爬虫 - Web版本

基于Crawl4AI的可视化网站内容抓取工具，提供友好的Web界面和实时操作体验。

## 🌟 Web版本特色

### 🎨 现代化界面设计
- **响应式布局** - 支持桌面和移动设备
- **实时进度显示** - WebSocket实时通信
- **可视化配置** - 图形化参数设置
- **结果展示** - 多种格式的结果查看

### ⚡ 实时功能
- **即时反馈** - 实时显示抓取进度和状态
- **动态日志** - 详细的操作日志实时更新
- **进度条** - 可视化显示任务完成度
- **统计信息** - 实时更新抓取统计数据

### 🛠️ 强大功能
- **全功能支持** - 支持Crawl4AI的所有核心功能
- **智能提取** - 支持CSS、XPath、LLM等多种提取策略
- **批量处理** - 高效的并发抓取机制
- **结果导出** - 多种格式的结果输出

## 🚀 快速开始

### 1. 环境准备

#### 系统要求
- Python 3.8+
- 4GB+ 内存
- 稳定的网络连接

#### 安装依赖
```bash
# 进入Web目录
cd tools/web-crawler/web

# 安装依赖包
pip install -r requirements.txt

# 安装Playwright浏览器（首次使用）
playwright install chromium
```

### 2. 启动服务

```bash
# 启动Web服务器
python start.py
```

启动成功后，您将看到：
```
🚀 智能网站爬虫 Web 服务器启动成功！
📍 Web界面: http://localhost:5000
🔧 API接口: http://localhost:5000/api
📊 实时通信: WebSocket已启用
```

### 3. 访问界面

打开浏览器，访问：`http://localhost:5000`

## 📋 界面功能详解

### 主要区域

#### 1. 配置面板（左侧）
**基础设置**
- **目标URL** - 输入要抓取的网站地址
- **输出目录** - 设置结果保存位置
- **爬取深度** - 控制抓取的层级深度
- **最大页面** - 限制抓取的页面数量
- **批处理大小** - 设置并发抓取数量

**爬取策略**
- `BFS (广度优先)` - 逐层抓取，适合完整抓取
- `DFS (深度优先)` - 深入抓取，适合特定路径
- `首页链接` - 仅抓取首页发现的链接

**输出格式**
- ✅ Markdown - 清洁的文本格式
- ✅ JSON - 结构化数据格式
- ✅ HTML索引 - 可视化浏览界面
- ✅ 截图 - 页面截图（可选）

#### 2. 高级选项（展开面板）
**过滤设置**
- 排除外部链接
- 排除社交媒体链接
- 排除图片文件链接
- 处理iframe内容

**浏览器配置**
- 浏览器类型选择
- 无头模式设置
- User-Agent配置
- 代理服务器设置

**提取策略**
- CSS选择器提取
- XPath表达式提取
- LLM智能提取
- 正则表达式提取

#### 3. 控制面板（右上）
- **开始抓取** - 启动爬虫任务
- **停止抓取** - 中止当前任务
- **清空日志** - 清理日志显示
- **导出结果** - 下载抓取结果

#### 4. 实时监控（右侧）
**进度显示**
- 圆形进度条显示完成百分比
- 实时更新任务状态
- 彩色状态指示器

**统计信息**
- 已发现页面数量
- 成功抓取页面数量
- 失败页面数量
- 当前抓取速度

**实时日志**
- 详细的操作日志
- 不同级别的日志颜色
- 自动滚动到最新日志
- 时间戳显示

#### 5. 结果展示（底部）
**页面列表**
- 表格形式显示抓取的页面
- 支持搜索和过滤
- 点击查看页面详情
- 状态图标显示

**导航结构**
- 树形显示网站结构
- 按层级组织页面
- 可展开/折叠节点
- 快速导航定位

## ⚙️ 使用技巧

### 1. 快速配置模板

#### 技术文档网站
```
目标URL: https://docs.example.com
爬取深度: 2-3
最大页面: 50-100
策略: BFS广度优先
输出: Markdown + JSON + HTML索引
```

#### 新闻网站
```
目标URL: https://news.example.com  
爬取深度: 1
最大页面: 20-30
策略: 首页链接
过滤: 排除外部链接、社交媒体
```

#### 电商网站
```
目标URL: https://shop.example.com
爬取深度: 2
最大页面: 100-200
提取策略: CSS选择器
延迟: 2-3秒
```

### 2. CSS选择器配置示例

#### 新闻文章提取
```json
{
  "name": "NewsArticle",
  "baseSelector": "article",
  "fields": [
    {"name": "title", "selector": "h1", "type": "text"},
    {"name": "author", "selector": ".author", "type": "text"},
    {"name": "content", "selector": ".article-body", "type": "text"},
    {"name": "date", "selector": ".publish-date", "type": "text"}
  ]
}
```

#### 产品页面提取
```json
{
  "name": "Product",
  "baseSelector": ".product-detail",
  "fields": [
    {"name": "name", "selector": ".product-title", "type": "text"},
    {"name": "price", "selector": ".price", "type": "text"},
    {"name": "description", "selector": ".description", "type": "text"},
    {"name": "images", "selector": ".product-images img", "type": "attribute", "attribute": "src"}
  ]
}
```

### 3. LLM提取配置

#### OpenAI配置
- **提供商**: `openai/gpt-4o-mini`
- **API密钥**: 输入您的OpenAI API密钥
- **提取指令**: 
```
请提取页面中的关键信息：
1. 主要标题和副标题
2. 核心内容摘要
3. 重要的数据和统计信息
4. 联系方式和链接

以JSON格式返回结果
```

#### Anthropic配置
- **提供商**: `anthropic/claude-3-sonnet`
- **API密钥**: 输入您的Anthropic API密钥
- **提取指令**:
```
分析网页内容并提取：
- 文档结构和章节
- 关键概念和定义
- 代码示例和技术细节
- 相关资源和参考链接

返回结构化的JSON数据
```

## 🔧 故障排除

### 常见问题

#### 1. 服务启动失败
**解决步骤：**
```bash
# 检查依赖安装
pip install -r requirements.txt

# 检查端口占用
netstat -ano | findstr :5000

# 查看详细错误
python start.py
```

#### 2. WebSocket连接失败
**可能原因：**
- 防火墙阻止连接
- 浏览器安全设置
- 代理服务器干扰

**解决方案：**
- 尝试使用 `http://127.0.0.1:5000`
- 检查浏览器控制台错误
- 暂时关闭代理

#### 3. 抓取任务失败
**排查步骤：**
1. 检查目标URL是否可访问
2. 查看实时日志的错误信息
3. 降低并发数和增加延迟
4. 检查网络连接状态

#### 4. 内存占用过高
**优化方案：**
- 减少批处理大小（设为2-5）
- 限制最大页面数
- 禁用截图功能
- 定期重启服务

### 错误代码说明

| 错误代码 | 含义 | 解决方案 |
|----------|------|----------|
| `NetworkError` | 网络连接问题 | 检查网络连接 |
| `TimeoutError` | 请求超时 | 增加延迟时间 |
| `ParseError` | 页面解析失败 | 检查页面结构 |
| `AuthError` | 认证失败 | 配置正确的认证信息 |
| `RateLimitError` | 请求频率过高 | 降低并发数 |

## 📊 性能优化

### 速度优化
1. **增加并发数** - 批处理大小设为8-15
2. **减少延迟** - 延迟时间设为0.5-1秒
3. **启用缓存** - 避免重复抓取
4. **选择合适策略** - BFS适合完整抓取

### 质量优化
1. **使用CSS选择器** - 精确提取目标内容
2. **配置过滤器** - 排除无关页面
3. **设置等待条件** - 确保页面完全加载
4. **验证结果** - 检查抓取数据的完整性

### 稳定性优化
1. **错误重试** - 启用自动重试机制
2. **监控日志** - 实时观察运行状态
3. **资源限制** - 合理设置资源使用
4. **定期维护** - 清理缓存和日志

## 🔗 相关链接

- **主项目文档**: [../README.md](../README.md)
- **Crawl4AI官网**: https://github.com/unclecode/crawl4ai
- **Playwright文档**: https://playwright.dev/
- **Flask文档**: https://flask.palletsprojects.com/

## 📝 更新日志

### v1.0.0 (当前版本)
- ✅ 完整的Web界面实现
- ✅ 实时进度显示和日志
- ✅ 支持所有Crawl4AI功能
- ✅ 多种提取策略支持
- ✅ 可视化结果展示

### 计划更新
- 🔄 任务队列管理
- 🔄 更多主题和界面选项
- 🔄 导出功能增强
- 🔄 性能监控面板

---

<div align="center">
  <p><strong>智能网站爬虫 Web版本</strong></p>
  <p>让网站内容抓取变得简单而强大 🚀</p>
</div> 