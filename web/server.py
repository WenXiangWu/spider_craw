#!/usr/bin/env python3
"""
æ™ºèƒ½ç½‘ç«™çˆ¬è™« Web æœåŠ¡å™¨
æä¾› REST API å’Œ WebSocket æ”¯æŒ
"""

import asyncio
import json
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import threading
import queue
import time

from flask import Flask, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
import logging

# çˆ¬è™«ç›¸å…³å¯¼å…¥
try:
    from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
    from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, LLMExtractionStrategy
    from crawl4ai.deep_crawling import BFSDeepCrawlStrategy, DFSDeepCrawlStrategy
    from crawl4ai.deep_crawling.filters import DomainFilter, URLPatternFilter, FilterChain
    from crawl4ai import LLMConfig
    CRAWL4AI_AVAILABLE = True
except ImportError:
    CRAWL4AI_AVAILABLE = False
    print("è­¦å‘Š: crawl4ai æœªå®‰è£…ï¼Œè¯·è¿è¡Œ 'pip install crawl4ai'")

# å†…å®¹è¿‡æ»¤å™¨å¯¼å…¥
try:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    from content_filter import ContentFilter, create_default_filter
    CONTENT_FILTER_AVAILABLE = True
    print("âœ… å†…å®¹è¿‡æ»¤å™¨å·²åŠ è½½")
except ImportError:
    CONTENT_FILTER_AVAILABLE = False
    print("è­¦å‘Š: å†…å®¹è¿‡æ»¤å™¨ä¸å¯ç”¨")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆå§‹åŒ– Flask åº”ç”¨
app = Flask(__name__, static_folder='.')
app.config['SECRET_KEY'] = 'your-secret-key'
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# å…¨å±€å˜é‡
tasks: Dict[str, Dict] = {}  # å­˜å‚¨ä»»åŠ¡çŠ¶æ€
task_queue = queue.Queue()   # ä»»åŠ¡é˜Ÿåˆ—
results_storage: Dict[str, List] = {}  # ç»“æœå­˜å‚¨


class CrawlerTask:
    """çˆ¬è™«ä»»åŠ¡ç±»"""
    
    def __init__(self, task_id: str, config: Dict):
        self.task_id = task_id
        self.config = config
        self.status = 'pending'
        self.progress = 0
        self.status_text = 'ç­‰å¾…å¼€å§‹...'
        self.stats = {
            'discovered': 0,
            'crawled': 0,
            'failed': 0,
            'filtered': 0
        }
        self.results = []
        self.navigation = []
        self.error = None
        self.start_time = None
        self.end_time = None
        
        # å­˜å‚¨å‘ç°çš„URLå’Œå†…å®¹
        self.discovered_urls = set()
        self.crawled_content = {}

    def update_status(self, status: str, progress: int = None, status_text: str = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        self.status = status
        if progress is not None:
            self.progress = progress
        if status_text:
            self.status_text = status_text
            
        # é€šè¿‡WebSocketå‘é€æ›´æ–°
        socketio.emit('task_update', {
            'task_id': self.task_id,
            'status': self.status,
            'progress': self.progress,
            'status_text': self.status_text,
            'stats': self.stats
        })

    def add_log(self, message: str, level: str = 'info'):
        """æ·»åŠ æ—¥å¿—"""
        log_entry = {
            'timestamp': datetime.now().isoformat(),
            'message': message,
            'level': level
        }
        
        socketio.emit('log', log_entry)
        logger.info(f"[{self.task_id}] {message}")

    def add_result(self, result: Dict):
        """æ·»åŠ ç»“æœ"""
        self.results.append(result)
        self.stats['crawled'] = len(self.results)
        
        socketio.emit('result', {
            'task_id': self.task_id,
            'result': result
        })

    async def run(self):
        """æ‰§è¡Œçˆ¬è™«ä»»åŠ¡"""
        if not CRAWL4AI_AVAILABLE:
            self.status = 'failed'
            self.error = 'crawl4ai æœªå®‰è£…'
            return

        try:
            self.start_time = datetime.now()
            self.update_status('running', 0, 'åˆå§‹åŒ–çˆ¬è™«...')
            self.add_log('å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡')

            # ç¬¬ä¸€æ­¥ï¼šå‘ç°ç½‘ç«™ç»“æ„
            await self.discover_website_structure()
            
            # ç¬¬äºŒæ­¥ï¼šæŠ“å–æ‰€æœ‰å†…å®¹
            await self.crawl_all_content()
            
            # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå¯¼èˆªç»“æ„
            self.generate_navigation_structure()
            
            # å®Œæˆ
            self.end_time = datetime.now()
            self.update_status('completed', 100, 'æŠ“å–å®Œæˆ')
            self.add_log('çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå®Œæˆ', 'success')
            
        except Exception as e:
            self.status = 'failed'
            self.error = str(e)
            self.add_log(f'ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}', 'error')
            logger.error(f"ä»»åŠ¡ {self.task_id} å¤±è´¥: {str(e)}", exc_info=True)

    async def discover_website_structure(self):
        """å‘ç°ç½‘ç«™ç»“æ„"""
        self.update_status('running', 10, 'å‘ç°ç½‘ç«™ç»“æ„...')
        self.add_log('å¼€å§‹å‘ç°ç½‘ç«™ç»“æ„')
        
        config = self.config
        browser_config = self._create_browser_config()
        
        # é…ç½®è¿‡æ»¤å™¨
        filters = []
        if config['filters']['exclude_domains']:
            domain_filter = DomainFilter(
                blocked_domains=config['filters']['exclude_domains']
            )
            filters.append(domain_filter)
        
        if config['filters']['exclude_patterns']:
            pattern_filter = URLPatternFilter(
                excluded_patterns=config['filters']['exclude_patterns']
            )
            filters.append(pattern_filter)
        
        # æ€»æ˜¯åˆ›å»º FilterChainï¼Œå³ä½¿æ²¡æœ‰è¿‡æ»¤å™¨ä¹Ÿåˆ›å»ºç©ºçš„
        filter_chain = FilterChain(filters)
        
        # é…ç½®æ·±åº¦çˆ¬å–ç­–ç•¥
        deep_crawl_strategy = None
        if config['crawl_strategy'] == 'bfs':
            deep_crawl_strategy = BFSDeepCrawlStrategy(
                max_depth=config['max_depth'],
                include_external=False,
                max_pages=config['max_pages'],
                filter_chain=filter_chain
            )
        elif config['crawl_strategy'] == 'dfs':
            deep_crawl_strategy = DFSDeepCrawlStrategy(
                max_depth=config['max_depth'],
                include_external=False,
                max_pages=config['max_pages'],
                filter_chain=filter_chain
            )
        
        # é…ç½®çˆ¬å–å‚æ•°
        run_config = CrawlerRunConfig(
            cache_mode=getattr(CacheMode, config['cache_mode']),
            deep_crawl_strategy=deep_crawl_strategy,
            exclude_external_links=config['filters']['exclude_external'],
            exclude_social_media_links=config['filters']['exclude_social'],
            exclude_external_images=config['filters']['exclude_images'],
            process_iframes=config['filters']['process_iframes'],
            word_count_threshold=config['word_threshold'],
            wait_for=config['wait_for'],
            stream=True
        )
        
        discovered_urls = [config['target_url']]
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            try:
                if deep_crawl_strategy:
                    # ä½¿ç”¨æ·±åº¦çˆ¬å–
                    async for result in await crawler.arun(
                        url=config['target_url'],
                        config=run_config
                    ):
                        if result.success:
                            current_url = result.url
                            discovered_urls.append(current_url)
                            self.discovered_urls.add(current_url)
                            
                            # æå–å†…éƒ¨é“¾æ¥
                            internal_links = result.links.get("internal", [])
                            for link in internal_links:
                                link_url = link.get("href", "")
                                if link_url and self._is_valid_url(link_url, config['target_url']):
                                    discovered_urls.append(link_url)
                                    self.discovered_urls.add(link_url)
                            
                            self.add_log(f'å‘ç°é¡µé¢: {current_url} (å†…éƒ¨é“¾æ¥: {len(internal_links)})')
                        else:
                            self.add_log(f'é¡µé¢å‘ç°å¤±è´¥: {result.url}', 'warning')
                else:
                    # ä»…æŠ“å–é¦–é¡µé“¾æ¥
                    result = await crawler.arun(url=config['target_url'], config=run_config)
                    if result.success:
                        internal_links = result.links.get("internal", [])
                        for link in internal_links:
                            link_url = link.get("href", "")
                            if link_url and self._is_valid_url(link_url, config['target_url']):
                                discovered_urls.append(link_url)
                                self.discovered_urls.add(link_url)
                        
                        self.add_log(f'åœ¨é¦–é¡µå‘ç° {len(internal_links)} ä¸ªå†…éƒ¨é“¾æ¥')
                    
            except Exception as e:
                self.add_log(f'ç½‘ç«™ç»“æ„å‘ç°å‡ºé”™: {str(e)}', 'error')
                raise
        
        # å»é‡å¹¶æ’åº
        unique_urls = list(set(discovered_urls))
        unique_urls.sort()
        
        self.stats['discovered'] = len(unique_urls)
        self.update_status('running', 30, f'å‘ç° {len(unique_urls)} ä¸ªé¡µé¢')
        self.add_log(f'ç½‘ç«™ç»“æ„å‘ç°å®Œæˆï¼Œå…±å‘ç° {len(unique_urls)} ä¸ªé¡µé¢')
        
        return unique_urls

    async def crawl_all_content(self):
        """æŠ“å–æ‰€æœ‰å†…å®¹"""
        urls = list(self.discovered_urls)
        if not urls:
            urls = [self.config['target_url']]
        
        self.update_status('running', 40, f'å¼€å§‹æŠ“å– {len(urls)} ä¸ªé¡µé¢çš„å†…å®¹...')
        self.add_log(f'å¼€å§‹æ‰¹é‡æŠ“å–å†…å®¹ï¼Œå…± {len(urls)} ä¸ªé¡µé¢')
        
        browser_config = self._create_browser_config()
        extraction_strategy = self._create_extraction_strategy()
        
        run_config = CrawlerRunConfig(
            cache_mode=getattr(CacheMode, self.config['cache_mode']),
            extraction_strategy=extraction_strategy,
            exclude_external_links=self.config['filters']['exclude_external'],
            exclude_social_media_links=self.config['filters']['exclude_social'],
            word_count_threshold=self.config['word_threshold'],
            screenshot='screenshot' in self.config['output_formats'],
            pdf='pdf' in self.config['output_formats']
        )
        
        successful_crawls = 0
        failed_crawls = 0
        batch_size = self.config['batch_size']
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            for i in range(0, len(urls), batch_size):
                batch_urls = urls[i:i + batch_size]
                batch_num = i // batch_size + 1
                total_batches = (len(urls) + batch_size - 1) // batch_size
                
                progress = 40 + int((i / len(urls)) * 50)
                self.update_status('running', progress, f'å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}')
                self.add_log(f'å¤„ç†æ‰¹æ¬¡ {batch_num}/{total_batches}')
                
                try:
                    results = await crawler.arun_many(urls=batch_urls, config=run_config)
                    
                    for result in results:
                        if result.success:
                            content_data = self._process_crawl_result(result)
                            self.add_result(content_data)
                            successful_crawls += 1
                            self.add_log(f'æˆåŠŸæŠ“å–: {result.url}')
                        else:
                            failed_crawls += 1
                            self.add_log(f'æŠ“å–å¤±è´¥: {result.url}', 'warning')
                
                except Exception as e:
                    self.add_log(f'æ‰¹æ¬¡å¤„ç†å‡ºé”™: {str(e)}', 'error')
                    failed_crawls += len(batch_urls)
                
                # çŸ­æš‚å»¶è¿Ÿ
                await asyncio.sleep(self.config['delay'])
        
        self.stats['crawled'] = successful_crawls
        self.stats['failed'] = failed_crawls
        
        self.update_status('running', 90, 'å¤„ç†æŠ“å–ç»“æœ...')
        self.add_log(f'å†…å®¹æŠ“å–å®Œæˆ! æˆåŠŸ: {successful_crawls}, å¤±è´¥: {failed_crawls}')

    def generate_navigation_structure(self):
        """ç”Ÿæˆå¯¼èˆªç»“æ„"""
        self.add_log('ç”Ÿæˆå¯¼èˆªç»“æ„...')
        
        # æå–çœŸå®çš„å¯¼èˆªæ å†…å®¹
        navigation = []
        navigation_items = set()  # ç”¨äºå»é‡
        
        for result in self.results:
            # ä»æå–çš„å†…å®¹ä¸­è·å–å¯¼èˆªæ ä¿¡æ¯
            extracted_content = result.get('extracted_content', {})
            
            # å¤„ç†å¯¼èˆªæ å†…å®¹
            nav_content = None
            if isinstance(extracted_content, list) and len(extracted_content) > 0:
                nav_content = extracted_content[0].get('navigation', '')
            elif isinstance(extracted_content, dict):
                nav_content = extracted_content.get('navigation', '')
            
            # å¦‚æœæœ‰å¯¼èˆªæ å†…å®¹ï¼Œè§£æå…¶ä¸­çš„é“¾æ¥
            if nav_content:
                nav_links = self._extract_navigation_links(nav_content, result['url'])
                for link in nav_links:
                    link_key = f"{link['title']}|{link['url']}"
                    if link_key not in navigation_items:
                        navigation_items.add(link_key)
                        navigation.append(link)
            
            # åŒæ—¶ä¿ç•™åŸºäºURLè·¯å¾„çš„å¯¼èˆªé¡¹
            nav_item = {
                'url': result['url'],
                'title': result['title'] or 'æ— æ ‡é¢˜',
                'path': result['url'].replace(self.config['target_url'], ''),
                'level': result['url'].count('/') - 2,  # ä¼°ç®—å±‚çº§
                'type': 'page'  # æ ‡è®°ä¸ºé¡µé¢ç±»å‹
            }
            navigation.append(nav_item)
        
        # æŒ‰ç…§å±‚çº§å’Œè·¯å¾„æ’åº
        navigation.sort(key=lambda x: (x.get('level', 0), x.get('path', x['url'])))
        self.navigation = navigation

    def _create_browser_config(self) -> BrowserConfig:
        """åˆ›å»ºæµè§ˆå™¨é…ç½®"""
        browser_config = self.config['browser']
        
        # ç¡®ä¿ user_agent ä¸ä¸º Noneï¼Œæä¾›é»˜è®¤å€¼
        user_agent = browser_config.get('user_agent')
        if not user_agent:
            user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        
        return BrowserConfig(
            headless=browser_config.get('headless', True),
            verbose=browser_config.get('verbose', False),
            user_agent=user_agent,
            proxy=browser_config.get('proxy')
        )

    def _create_extraction_strategy(self):
        """åˆ›å»ºæå–ç­–ç•¥"""
        extraction = self.config['extraction']
        
        if extraction['type'] == 'css' and extraction['css_selectors']:
            return JsonCssExtractionStrategy(extraction['css_selectors'])
        elif extraction['type'] == 'llm' and extraction['llm_api_key']:
            llm_config = LLMConfig(
                provider=extraction['llm_provider'],
                api_token=extraction['llm_api_key']
            )
            return LLMExtractionStrategy(
                llm_config=llm_config,
                instruction=extraction['llm_instruction'] or "æå–é¡µé¢çš„ä¸»è¦å†…å®¹",
                extraction_type="json"
            )
        
        return None

    def _process_crawl_result(self, result) -> Dict:
        """å¤„ç†çˆ¬å–ç»“æœ"""
        content_data = {
            'url': result.url,
            'title': '',
            'description': '',
            'timestamp': datetime.now().isoformat(),
            'status_code': result.status_code,
            'markdown': str(result.markdown) if result.markdown else '',
            'links_count': len(result.links.get('internal', [])),
            'extracted_content': {}
        }
        
        # å¤„ç†æå–çš„ç»“æ„åŒ–å†…å®¹
        if result.extracted_content:
            try:
                extracted = json.loads(result.extracted_content)
                content_data['extracted_content'] = extracted
                
                # æå–æ ‡é¢˜å’Œæè¿°
                if extracted and len(extracted) > 0:
                    first_item = extracted[0] if isinstance(extracted, list) else extracted
                    content_data['title'] = first_item.get('title', '')
                    content_data['description'] = first_item.get('description', '')
            except json.JSONDecodeError:
                pass
        
        # å¦‚æœæ²¡æœ‰ä»æå–å†…å®¹ä¸­è·å¾—æ ‡é¢˜ï¼Œå°è¯•ä»URLæ¨æ–­
        if not content_data['title']:
            content_data['title'] = result.url.split('/')[-1] or 'Home'
        
        return content_data

    def _extract_navigation_links(self, nav_content: str, base_url: str) -> List[Dict]:
        """ä»å¯¼èˆªæ å†…å®¹ä¸­æå–é“¾æ¥"""
        import re
        from urllib.parse import urljoin, urlparse
        
        nav_links = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–é“¾æ¥
        # åŒ¹é… markdown é“¾æ¥æ ¼å¼ [text](url)
        markdown_links = re.findall(r'\[([^\]]+)\]\(([^)]+)\)', nav_content)
        for text, url in markdown_links:
            full_url = urljoin(base_url, url)
            if self._is_valid_url(full_url, self.config['target_url']):
                nav_links.append({
                    'title': text.strip(),
                    'url': full_url,
                    'type': 'navigation',
                    'level': 0
                })
        
        # åŒ¹é… HTML é“¾æ¥æ ¼å¼ <a href="url">text</a>
        html_links = re.findall(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>', nav_content, re.IGNORECASE)
        for url, text in html_links:
            full_url = urljoin(base_url, url)
            if self._is_valid_url(full_url, self.config['target_url']):
                nav_links.append({
                    'title': text.strip(),
                    'url': full_url,
                    'type': 'navigation',
                    'level': 0
                })
        
        # å»é‡
        unique_links = []
        seen = set()
        for link in nav_links:
            key = f"{link['title']}|{link['url']}"
            if key not in seen:
                seen.add(key)
                unique_links.append(link)
        
        return unique_links

    def _is_valid_url(self, url: str, base_url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            from urllib.parse import urlparse
            base_parsed = urlparse(base_url)
            url_parsed = urlparse(url)
            
            return (
                url_parsed.netloc == base_parsed.netloc and
                url_parsed.scheme in ['http', 'https'] and
                not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.css', '.js'])
            )
        except:
            return False

    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'task_id': self.task_id,
            'status': self.status,
            'progress': self.progress,
            'status_text': self.status_text,
            'stats': self.stats,
            'results': self.results,
            'navigation': self.navigation,
            'error': self.error,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None
        }


def task_worker():
    """ä»»åŠ¡å·¥ä½œçº¿ç¨‹"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    
    while True:
        try:
            task = task_queue.get(timeout=1)
            if task is None:  # é€€å‡ºä¿¡å·
                break
            
            # è¿è¡Œå¼‚æ­¥ä»»åŠ¡
            loop.run_until_complete(task.run())
            
        except queue.Empty:
            continue
        except Exception as e:
            logger.error(f"ä»»åŠ¡å·¥ä½œçº¿ç¨‹é”™è¯¯: {str(e)}", exc_info=True)


# å¯åŠ¨å·¥ä½œçº¿ç¨‹
worker_thread = threading.Thread(target=task_worker, daemon=True)
worker_thread.start()


@app.route('/')
def index():
    """ä¸»é¡µ"""
    return send_from_directory('.', 'index.html')


@app.route('/<path:filename>')
def static_files(filename):
    """é™æ€æ–‡ä»¶"""
    return send_from_directory('.', filename)


@app.route('/api/crawl', methods=['POST'])
def start_crawl():
    """å¯åŠ¨çˆ¬å–ä»»åŠ¡"""
    try:
        config = request.get_json()
        
        # éªŒè¯é…ç½®
        if not config or not config.get('target_url'):
            return jsonify({'success': False, 'error': 'ç¼ºå°‘ç›®æ ‡URL'}), 400
        
        # åˆ›å»ºä»»åŠ¡
        task_id = str(uuid.uuid4())
        task = CrawlerTask(task_id, config)
        tasks[task_id] = task
        
        # æ·»åŠ åˆ°é˜Ÿåˆ—
        task_queue.put(task)
        
        return jsonify({
            'success': True,
            'task_id': task_id,
            'message': 'çˆ¬å–ä»»åŠ¡å·²å¯åŠ¨'
        })
        
    except Exception as e:
        logger.error(f"å¯åŠ¨çˆ¬å–ä»»åŠ¡å¤±è´¥: {str(e)}", exc_info=True)
        return jsonify({'success': False, 'error': str(e)}), 500


@app.route('/api/status/<task_id>')
def get_task_status(task_id):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    return jsonify(task.to_dict())


@app.route('/api/results/<task_id>')
def get_task_results(task_id):
    """è·å–ä»»åŠ¡ç»“æœ"""
    task = tasks.get(task_id)
    if not task:
        return jsonify({'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    return jsonify({
        'results': task.results,
        'navigation': task.navigation,
        'stats': task.stats
    })


@app.route('/api/tasks')
def list_tasks():
    """åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡"""
    task_list = []
    for task_id, task in tasks.items():
        task_list.append({
            'task_id': task_id,
            'status': task.status,
            'progress': task.progress,
            'stats': task.stats,
            'start_time': task.start_time.isoformat() if task.start_time else None
        })
    
    return jsonify({'tasks': task_list})


@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥"""
    emit('connected', {'message': 'WebSocketè¿æ¥å·²å»ºç«‹'})


@socketio.on('disconnect')
def handle_disconnect():
    """WebSocketæ–­å¼€"""
    print('å®¢æˆ·ç«¯æ–­å¼€è¿æ¥')


if __name__ == '__main__':
    print("ğŸš€ å¯åŠ¨æ™ºèƒ½ç½‘ç«™çˆ¬è™« Web æœåŠ¡å™¨...")
    print("ğŸ“± Webç•Œé¢: http://localhost:5000")
    print("ğŸ”Œ APIæ–‡æ¡£: http://localhost:5000/api/tasks")
    
    if not CRAWL4AI_AVAILABLE:
        print("âš ï¸  è­¦å‘Š: crawl4ai æœªå®‰è£…ï¼Œè¯·è¿è¡Œ 'pip install crawl4ai'")
    
    socketio.run(app, host='0.0.0.0', port=5000, debug=True) 