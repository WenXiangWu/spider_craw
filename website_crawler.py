#!/usr/bin/env python3
"""
ç½‘ç«™å†…å®¹æŠ“å–å™¨
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨å‘ç°ç½‘ç«™çš„æ‰€æœ‰å­é¡µé¢ï¼ˆå¯¼èˆªæ ã€é“¾æ¥ç­‰ï¼‰
2. æ‰¹é‡æŠ“å–æ‰€æœ‰é¡µé¢å†…å®¹å¹¶ç»“æ„åŒ–å­˜å‚¨
3. å»ºç«‹URLä¸å†…å®¹çš„æ˜ å°„å…³ç³»

ä½œè€…ï¼šAITOOLBOX
åˆ›å»ºæ—¶é—´ï¼š2025å¹´1æœˆ
"""

import asyncio
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from urllib.parse import urljoin, urlparse
import logging

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.deep_crawling.filters import DomainFilter, URLPatternFilter, FilterChain

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥å¢å¼ºå¯¼èˆªæå–å™¨
try:
    from enhanced_navigation_extractor import EnhancedNavigationExtractor, enhance_navigation_extraction
    ENHANCED_NAV_AVAILABLE = True
    logger.info("âœ… å¢å¼ºå¯¼èˆªæå–å™¨å·²åŠ è½½")
except ImportError:
    ENHANCED_NAV_AVAILABLE = False
    logger.warning("âš ï¸  å¢å¼ºå¯¼èˆªæå–å™¨ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨åŸºç¡€å¯¼èˆªåŠŸèƒ½")

# å°è¯•å¯¼å…¥å†…å®¹è¿‡æ»¤å™¨
try:
    from content_filter import ContentFilter, create_default_filter
    CONTENT_FILTER_AVAILABLE = True
    logger.info("âœ… å†…å®¹è¿‡æ»¤å™¨å·²åŠ è½½")
except ImportError:
    CONTENT_FILTER_AVAILABLE = False
    logger.warning("âš ï¸  å†…å®¹è¿‡æ»¤å™¨ä¸å¯ç”¨ï¼Œå°†ä¸è¿›è¡Œå†…å®¹è¿‡æ»¤")


class WebsiteCrawler:
    """ç½‘ç«™çˆ¬è™«ç±»ï¼Œè´Ÿè´£å‘ç°å’ŒæŠ“å–ç½‘ç«™å†…å®¹"""
    
    def __init__(self, base_url: str, output_dir: str = "results", 
                 content_filter_config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–çˆ¬è™«
        
        Args:
            base_url: ç›®æ ‡ç½‘ç«™çš„åŸºç¡€URL
            output_dir: è¾“å‡ºç›®å½•
            content_filter_config: å†…å®¹è¿‡æ»¤å™¨é…ç½®
        """
        self.base_url = base_url.rstrip('/')
        self.domain = urlparse(base_url).netloc
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # å­˜å‚¨å‘ç°çš„URLå’Œå†…å®¹
        self.discovered_urls: Set[str] = set()
        self.crawled_content: Dict[str, Dict] = {}
        self.url_mapping: Dict[str, str] = {}  # URLåˆ°æ–‡ä»¶è·¯å¾„çš„æ˜ å°„
        
        # åˆå§‹åŒ–å†…å®¹è¿‡æ»¤å™¨
        self.content_filter = None
        if CONTENT_FILTER_AVAILABLE and content_filter_config:
            self.content_filter = ContentFilter.create_from_config(content_filter_config)
            logger.info("âœ… å†…å®¹è¿‡æ»¤å™¨å·²å¯ç”¨")
        elif CONTENT_FILTER_AVAILABLE:
            # ä½¿ç”¨é»˜è®¤è¿‡æ»¤å™¨
            self.content_filter = create_default_filter()
            logger.info("âœ… ä½¿ç”¨é»˜è®¤å†…å®¹è¿‡æ»¤å™¨")
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_discovered': 0,
            'total_crawled': 0,
            'failed_crawls': 0,
            'filtered_elements': 0,
            'start_time': None,
            'end_time': None
        }
    
    async def discover_website_structure(self, max_depth: int = 3, max_pages: int = 100) -> List[str]:
        """
        ç¬¬ä¸€æ­¥ï¼šå‘ç°ç½‘ç«™ç»“æ„ï¼Œè·å–æ‰€æœ‰å¯èƒ½çš„é¡µé¢URL
        
        Args:
            max_depth: æœ€å¤§çˆ¬å–æ·±åº¦
            max_pages: æœ€å¤§é¡µé¢æ•°é‡
            
        Returns:
            å‘ç°çš„æ‰€æœ‰URLåˆ—è¡¨
        """
        logger.info(f"ğŸ” å¼€å§‹å‘ç°ç½‘ç«™ç»“æ„: {self.base_url}")
        self.stats['start_time'] = datetime.now()
        
        # é…ç½®æµè§ˆå™¨
        browser_config = BrowserConfig(
            headless=True,
            verbose=False
        )
        
        # é…ç½®åŸŸåè¿‡æ»¤å™¨ï¼Œåªçˆ¬å–åŒåŸŸåé¡µé¢
        domain_filter = DomainFilter(
            allowed_domains=[self.domain],
            blocked_domains=[]
        )
        
        # é…ç½®URLæ¨¡å¼è¿‡æ»¤å™¨ï¼Œæ’é™¤ä¸éœ€è¦çš„æ–‡ä»¶ç±»å‹
        url_filter = URLPatternFilter(
            excluded_patterns=[
                "*.pdf", "*.jpg", "*.jpeg", "*.png", "*.gif", "*.svg",
                "*.css", "*.js", "*.ico", "*.woff", "*.woff2", "*.ttf",
                "*.zip", "*.tar", "*.gz", "*.mp4", "*.mp3", "*.avi"
            ]
        )
        
        filter_chain = FilterChain([domain_filter, url_filter])
        
        # é…ç½®æ·±åº¦çˆ¬å–ç­–ç•¥
        deep_crawl_strategy = BFSDeepCrawlStrategy(
            max_depth=max_depth,
            include_external=False,
            max_pages=max_pages,
            filter_chain=filter_chain
        )
        
        # é…ç½®çˆ¬å–å‚æ•°
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            deep_crawl_strategy=deep_crawl_strategy,
            exclude_external_links=True,
            exclude_social_media_links=True,
            exclude_domains=["facebook.com", "twitter.com", "instagram.com", "linkedin.com"],
            word_count_threshold=50,  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
            stream=True  # å¯ç”¨æµå¼å¤„ç†
        )
        
        discovered_urls = []
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            try:
                # ä½¿ç”¨æ·±åº¦çˆ¬å–å‘ç°æ‰€æœ‰é¡µé¢
                async for result in await crawler.arun(
                    url=self.base_url,
                    config=run_config
                ):
                    if result.success:
                        current_url = result.url
                        discovered_urls.append(current_url)
                        self.discovered_urls.add(current_url)
                        
                        # æå–é¡µé¢ä¸­çš„æ‰€æœ‰å†…éƒ¨é“¾æ¥
                        internal_links = result.links.get("internal", [])
                        for link in internal_links:
                            link_url = link.get("href", "")
                            if link_url and link_url not in self.discovered_urls:
                                full_url = urljoin(self.base_url, link_url)
                                if self._is_valid_url(full_url):
                                    discovered_urls.append(full_url)
                                    self.discovered_urls.add(full_url)
                        
                        logger.info(f"âœ… å‘ç°é¡µé¢: {current_url} (å†…éƒ¨é“¾æ¥: {len(internal_links)})")
                    else:
                        logger.warning(f"âŒ é¡µé¢å‘ç°å¤±è´¥: {result.url} - {result.error_message}")
            
            except Exception as e:
                logger.error(f"âŒ ç½‘ç«™ç»“æ„å‘ç°è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        
        # å»é‡å¹¶æ’åº
        unique_urls = list(set(discovered_urls))
        unique_urls.sort()
        
        self.stats['total_discovered'] = len(unique_urls)
        logger.info(f"ğŸ‰ ç½‘ç«™ç»“æ„å‘ç°å®Œæˆ! å…±å‘ç° {len(unique_urls)} ä¸ªé¡µé¢")
        
        # ä¿å­˜å‘ç°çš„URLåˆ—è¡¨
        self._save_discovered_urls(unique_urls)
        
        return unique_urls
    
    async def crawl_all_content(self, urls: List[str], batch_size: int = 10) -> Dict[str, Any]:
        """
        ç¬¬äºŒæ­¥ï¼šæ‰¹é‡æŠ“å–æ‰€æœ‰é¡µé¢å†…å®¹
        
        Args:
            urls: è¦æŠ“å–çš„URLåˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            
        Returns:
            æŠ“å–ç»“æœç»Ÿè®¡
        """
        logger.info(f"ğŸ“¥ å¼€å§‹æ‰¹é‡æŠ“å–å†…å®¹ï¼Œå…± {len(urls)} ä¸ªé¡µé¢")
        
        # é…ç½®æµè§ˆå™¨
        browser_config = BrowserConfig(
            headless=True,
            verbose=False
        )
        
        # é…ç½®å†…å®¹æå–ç­–ç•¥
        extraction_schema = {
            "name": "PageContent",
            "baseSelector": "body",
            "fields": [
                {"name": "title", "selector": "title, h1", "type": "text"},
                {"name": "description", "selector": "meta[name='description']", "type": "attribute", "attribute": "content"},
                {"name": "headings", "selector": "h1, h2, h3, h4, h5, h6", "type": "text"},
                {"name": "main_content", "selector": "main, article, .content, .main-content", "type": "text"},
                {"name": "navigation", "selector": "nav, .nav, .navigation, .navbar, .menu, .sidebar, [role='navigation'], .nav-menu, .main-nav, .site-nav, .primary-nav, .header-nav, .top-nav, .side-nav, .navigation-menu", "type": "html"},
                {"name": "navigation_links", "selector": "nav a, .nav a, .navigation a, .navbar a, .menu a, .sidebar a, [role='navigation'] a, .nav-menu a, .main-nav a, .site-nav a, .primary-nav a, .header-nav a, .top-nav a, .side-nav a, .navigation-menu a", "type": "text", "attribute": "href"}
            ]
        }
        
        extraction_strategy = JsonCssExtractionStrategy(extraction_schema)
        
        # é…ç½®çˆ¬å–å‚æ•°
        run_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            extraction_strategy=extraction_strategy,
            exclude_external_links=True,
            exclude_social_media_links=True,
            word_count_threshold=10,
            screenshot=False,  # å¯é€‰ï¼šæ˜¯å¦æˆªå›¾
            markdown_generator=None  # ä½¿ç”¨é»˜è®¤Markdownç”Ÿæˆå™¨
        )
        
        # åˆ†æ‰¹å¤„ç†URL
        successful_crawls = 0
        failed_crawls = 0
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            for i in range(0, len(urls), batch_size):
                batch_urls = urls[i:i + batch_size]
                logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(urls) + batch_size - 1)//batch_size}")
                
                try:
                    # æ‰¹é‡çˆ¬å–
                    results = await crawler.arun_many(
                        urls=batch_urls,
                        config=run_config
                    )
                    
                    # å¤„ç†ç»“æœ
                    for result in results:
                        if result.success:
                            await self._process_crawl_result(result)
                            successful_crawls += 1
                            logger.info(f"âœ… å†…å®¹æŠ“å–æˆåŠŸ: {result.url}")
                        else:
                            failed_crawls += 1
                            logger.warning(f"âŒ å†…å®¹æŠ“å–å¤±è´¥: {result.url} - {result.error_message}")
                
                except Exception as e:
                    logger.error(f"âŒ æ‰¹æ¬¡å¤„ç†å‡ºé”™: {str(e)}")
                    failed_crawls += len(batch_urls)
                
                # çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                await asyncio.sleep(1)
        
        self.stats['total_crawled'] = successful_crawls
        self.stats['failed_crawls'] = failed_crawls
        self.stats['end_time'] = datetime.now()
        
        logger.info(f"ğŸ‰ å†…å®¹æŠ“å–å®Œæˆ! æˆåŠŸ: {successful_crawls}, å¤±è´¥: {failed_crawls}")
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        await self._save_final_results()
        
        return {
            'successful_crawls': successful_crawls,
            'failed_crawls': failed_crawls,
            'total_content_items': len(self.crawled_content),
            'output_directory': str(self.output_dir.absolute())
        }
    
    async def _process_crawl_result(self, result) -> None:
        """å¤„ç†å•ä¸ªçˆ¬å–ç»“æœ"""
        url = result.url
        
        # ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        safe_filename = self._url_to_filename(url)
        
        # åº”ç”¨å†…å®¹è¿‡æ»¤
        cleaned_html = result.cleaned_html
        markdown_content = result.markdown.raw_markdown if hasattr(result.markdown, 'raw_markdown') else str(result.markdown)
        
        if self.content_filter:
            # è¿‡æ»¤HTMLå†…å®¹
            filtered_html = self.content_filter.filter_html(cleaned_html)
            if filtered_html != cleaned_html:
                self.stats['filtered_elements'] += 1
                logger.info(f"å·²å¯¹é¡µé¢ {url} åº”ç”¨å†…å®¹è¿‡æ»¤")
                cleaned_html = filtered_html
            
            # è¿‡æ»¤Markdownå†…å®¹
            filtered_markdown = self.content_filter.filter_text(markdown_content)
            if filtered_markdown != markdown_content:
                markdown_content = filtered_markdown
        
        # æå–å†…å®¹
        content_data = {
            'url': url,
            'title': '',
            'description': '',
            'timestamp': datetime.now().isoformat(),
            'status_code': result.status_code,
            'markdown': markdown_content,
            'cleaned_html': cleaned_html,
            'extracted_content': {},
            'links': result.links,
            'media': result.media,
            'content_filtered': bool(self.content_filter)
        }
        
        # å¤„ç†æå–çš„ç»“æ„åŒ–å†…å®¹
        if result.extracted_content:
            try:
                extracted = json.loads(result.extracted_content)
                content_data['extracted_content'] = extracted
                # æå–æ ‡é¢˜å’Œæè¿°
                if extracted and len(extracted) > 0:
                    first_item = extracted[0] if isinstance(extracted, list) else extracted
                    content_data['title'] = first_item.get('title', '')
                    content_data['description'] = first_item.get('description', '')
                    
                    # ä¸“é—¨å¤„ç†å¯¼èˆªæ å†…å®¹
                    nav_html = first_item.get('navigation', '')
                    nav_links = first_item.get('navigation_links', [])
                    
                    if nav_html or nav_links:
                        content_data['navigation_data'] = {
                            'html': nav_html,
                            'links': nav_links,
                            'extracted_links': self._extract_navigation_links(nav_html, url) if nav_html else []
                        }
            except json.JSONDecodeError:
                logger.warning(f"âš ï¸ æ— æ³•è§£ææå–çš„å†…å®¹: {url}")
        
        # ä¿å­˜åˆ°å†…å­˜
        self.crawled_content[url] = content_data
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        content_file = self.output_dir / "content" / f"{safe_filename}.json"
        content_file.parent.mkdir(exist_ok=True)
        
        with open(content_file, 'w', encoding='utf-8') as f:
            json.dump(content_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜Markdownæ–‡ä»¶
        markdown_file = self.output_dir / "markdown" / f"{safe_filename}.md"
        markdown_file.parent.mkdir(exist_ok=True)
        
        with open(markdown_file, 'w', encoding='utf-8') as f:
            f.write(f"# {content_data['title']}\n\n")
            f.write(f"**URL**: {url}\n")
            f.write(f"**æŠ“å–æ—¶é—´**: {content_data['timestamp']}\n\n")
            f.write("---\n\n")
            f.write(content_data['markdown'])
        
        # æ›´æ–°URLæ˜ å°„
        self.url_mapping[url] = str(content_file.relative_to(self.output_dir))
    
    def _extract_navigation_links(self, nav_html: str, base_url: str) -> List[Dict]:
        """ä»å¯¼èˆªæ HTMLä¸­æå–é“¾æ¥"""
        import re
        from urllib.parse import urljoin
        
        nav_links = []
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–HTMLé“¾æ¥
        html_links = re.findall(r'<a[^>]+href=["\']([^"\']+)["\'][^>]*>([^<]+)</a>', nav_html, re.IGNORECASE)
        for url, text in html_links:
            full_url = urljoin(base_url, url)
            if self._is_valid_url(full_url):
                nav_links.append({
                    'title': text.strip(),
                    'url': full_url,
                    'type': 'navigation'
                })
        
        # å»é‡
        unique_links = []
        seen = set()
        for link in nav_links:
            key = f"{link['title']}|{link['url']}"
            if key not in seen:
                seen.add(key)
                unique_links.append(link)
        
        return unique_links

    def _is_valid_url(self, url: str) -> bool:
        """æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆ"""
        try:
            parsed = urlparse(url)
            return (
                parsed.netloc == self.domain and
                parsed.scheme in ['http', 'https'] and
                not any(ext in url.lower() for ext in ['.pdf', '.jpg', '.png', '.gif', '.css', '.js'])
            )
        except:
            return False
    
    def _url_to_filename(self, url: str) -> str:
        """å°†URLè½¬æ¢ä¸ºå®‰å…¨çš„æ–‡ä»¶å"""
        parsed = urlparse(url)
        path = parsed.path.strip('/')
        if not path:
            path = 'index'
        
        # æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        safe_chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_"
        filename = ''.join(c if c in safe_chars else '_' for c in path)
        
        # é™åˆ¶é•¿åº¦
        if len(filename) > 100:
            filename = filename[:100]
        
        return filename or 'page'
    
    def _save_discovered_urls(self, urls: List[str]) -> None:
        """ä¿å­˜å‘ç°çš„URLåˆ—è¡¨"""
        urls_file = self.output_dir / "discovered_urls.json"
        urls_data = {
            'base_url': self.base_url,
            'discovery_time': datetime.now().isoformat(),
            'total_count': len(urls),
            'urls': urls
        }
        
        with open(urls_file, 'w', encoding='utf-8') as f:
            json.dump(urls_data, f, ensure_ascii=False, indent=2)
        
        # åŒæ—¶ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶ï¼Œä¾¿äºæŸ¥çœ‹
        txt_file = self.output_dir / "discovered_urls.txt"
        with open(txt_file, 'w', encoding='utf-8') as f:
            f.write(f"ç½‘ç«™: {self.base_url}\n")
            f.write(f"å‘ç°æ—¶é—´: {urls_data['discovery_time']}\n")
            f.write(f"æ€»è®¡: {len(urls)} ä¸ªé¡µé¢\n\n")
            for i, url in enumerate(urls, 1):
                f.write(f"{i:3d}. {url}\n")
    
    async def _save_final_results(self) -> None:
        """ä¿å­˜æœ€ç»ˆç»“æœå’Œç»Ÿè®¡ä¿¡æ¯"""
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_file = self.output_dir / "crawl_stats.json"
        duration = (self.stats['end_time'] - self.stats['start_time']).total_seconds() if self.stats['end_time'] and self.stats['start_time'] else 0
        
        final_stats = {
            **self.stats,
            'duration_seconds': duration,
            'start_time': self.stats['start_time'].isoformat() if self.stats['start_time'] else None,
            'end_time': self.stats['end_time'].isoformat() if self.stats['end_time'] else None,
            'base_url': self.base_url,
            'domain': self.domain,
            'enhanced_navigation_enabled': ENHANCED_NAV_AVAILABLE
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(final_stats, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜URLæ˜ å°„
        mapping_file = self.output_dir / "url_mapping.json"
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(self.url_mapping, f, ensure_ascii=False, indent=2)
        
        # å¦‚æœå¯ç”¨äº†å¢å¼ºå¯¼èˆªåŠŸèƒ½ï¼Œè¿›è¡Œå¯¼èˆªæ å¢å¼ºå¤„ç†
        if ENHANCED_NAV_AVAILABLE:
            await self._process_enhanced_navigation()
        
        # åˆ›å»ºç´¢å¼•æ–‡ä»¶
        await self._create_index_file()
    
    async def _process_enhanced_navigation(self) -> None:
        """å¤„ç†å¢å¼ºå¯¼èˆªåŠŸèƒ½"""
        if not ENHANCED_NAV_AVAILABLE:
            return
        
        logger.info("ğŸ” å¼€å§‹å¤„ç†å¢å¼ºå¯¼èˆªåŠŸèƒ½...")
        
        try:
            # å‡†å¤‡çˆ¬å–ç»“æœæ•°æ®
            crawl_results = []
            for url, content in self.crawled_content.items():
                crawl_result = {
                    'url': url,
                    'title': content.get('title', ''),
                    'html_content': content.get('html', ''),
                    'navigation': content.get('navigation', ''),
                    'navigation_links': content.get('navigation_links', [])
                }
                crawl_results.append(crawl_result)
            
            # ä½¿ç”¨å¢å¼ºå¯¼èˆªæå–å™¨å¤„ç†
            enhanced_results = enhance_navigation_extraction(crawl_results, self.base_url)
            
            # ä¿å­˜å¢å¼ºå¯¼èˆªç»“æœ
            enhanced_nav_file = self.output_dir / "enhanced_navigation.json"
            with open(enhanced_nav_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… å¢å¼ºå¯¼èˆªç»“æœå·²ä¿å­˜åˆ°: {enhanced_nav_file}")
            
            # ç”Ÿæˆå¯¼èˆªæŠ¥å‘Š
            await self._generate_navigation_report(enhanced_results)
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºå¯¼èˆªå¤„ç†å¤±è´¥: {str(e)}")
    
    async def _generate_navigation_report(self, enhanced_results: Dict[str, Any]) -> None:
        """ç”Ÿæˆå¯¼èˆªæ åˆ†ææŠ¥å‘Š"""
        report_file = self.output_dir / "navigation_report.html"
        
        report_html = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>å¯¼èˆªæ åˆ†ææŠ¥å‘Š - {self.domain}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; line-height: 1.6; }}
        .header {{ background: #f0f8ff; padding: 20px; border-radius: 8px; margin-bottom: 20px; }}
        .stats {{ background: #f9f9f9; padding: 15px; border-radius: 5px; margin-bottom: 20px; }}
        .nav-item {{ border: 1px solid #ddd; margin: 10px 0; padding: 15px; border-radius: 5px; }}
        .nav-item:hover {{ background: #f9f9f9; }}
        .nav-link {{ color: #1976d2; text-decoration: none; }}
        .nav-link:hover {{ text-decoration: underline; }}
        .level-0 {{ margin-left: 0; }}
        .level-1 {{ margin-left: 20px; }}
        .level-2 {{ margin-left: 40px; }}
        .level-3 {{ margin-left: 60px; }}
        .enhanced-badge {{ background: #4caf50; color: white; padding: 2px 6px; border-radius: 3px; font-size: 12px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>å¯¼èˆªæ åˆ†ææŠ¥å‘Š <span class="enhanced-badge">å¢å¼ºç‰ˆ</span></h1>
        <p><strong>ç½‘ç«™:</strong> {self.base_url}</p>
        <p><strong>åˆ†ææ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="stats">
        <h3>ç»Ÿè®¡ä¿¡æ¯</h3>
        <p>æ€»é¡µé¢æ•°: <strong>{enhanced_results.get('total_pages', 0)}</strong></p>
        <p>åŒ…å«å¯¼èˆªçš„é¡µé¢æ•°: <strong>{enhanced_results.get('pages_with_navigation', 0)}</strong></p>
        <p>å”¯ä¸€å¯¼èˆªé“¾æ¥æ•°: <strong>{len(enhanced_results.get('unique_navigation_links', []))}</strong></p>
        <p>å¢å¼ºå¯¼èˆªåŠŸèƒ½: <strong>âœ… å·²å¯ç”¨</strong></p>
    </div>
    
    <h2>å¯¼èˆªç»“æ„</h2>
    <div class="navigation-structure">
"""
        
        # æ·»åŠ å¯¼èˆªç»“æ„
        for item in enhanced_results.get('navigation_structure', []):
            level_class = f"level-{item.get('level', 0)}"
            report_html += f"""
        <div class="nav-item {level_class}">
            <a href="{item.get('url', '#')}" class="nav-link" target="_blank">
                {item.get('text', 'N/A')}
            </a>
            {f"<small> - å±‚çº§: {item.get('level', 0)}</small>" if item.get('level', 0) > 0 else ""}
        </div>
"""
        
        report_html += """
    </div>
    
    <h2>å¢å¼ºåŠŸèƒ½ç‰¹æ€§</h2>
    <div class="features">
        <ul>
            <li>âœ… 15ç§ä¸“ä¸šå¯¼èˆªé€‰æ‹©å™¨ï¼ˆvs åŸæ¥çš„3ç§ï¼‰</li>
            <li>âœ… å®Œæ•´HTMLç»“æ„ä¿ç•™ï¼ˆvs åŸæ¥çš„çº¯æ–‡æœ¬ï¼‰</li>
            <li>âœ… å¤šå±‚çº§å¯¼èˆªè¯†åˆ«</li>
            <li>âœ… æ™ºèƒ½é“¾æ¥æå–å’Œå»é‡</li>
            <li>âœ… å¯¼èˆªç»“æ„åˆ†æå’ŒæŠ¥å‘Š</li>
            <li>âœ… ä¸ç°æœ‰ç³»ç»Ÿå®Œå…¨å…¼å®¹</li>
        </ul>
    </div>
</body>
</html>
"""
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_html)
        
        logger.info(f"âœ… å¯¼èˆªæ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    async def _create_index_file(self) -> None:
        """åˆ›å»ºç´¢å¼•æ–‡ä»¶ï¼Œä¾¿äºæµè§ˆå’ŒæŸ¥æ‰¾"""
        index_file = self.output_dir / "index.html"
        
        html_content = f"""
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ç½‘ç«™å†…å®¹æŠ“å–ç»“æœ - {self.domain}</title>
    <style>
        body {{ font-family: Arial, sans-serif; margin: 20px; }}
        .header {{ background: #f5f5f5; padding: 20px; border-radius: 5px; margin-bottom: 20px; }}
        .stats {{ display: flex; gap: 20px; margin-bottom: 20px; }}
        .stat-item {{ background: #e3f2fd; padding: 10px; border-radius: 5px; text-align: center; }}
        .url-list {{ max-height: 600px; overflow-y: auto; border: 1px solid #ddd; }}
        .url-item {{ padding: 10px; border-bottom: 1px solid #eee; }}
        .url-item:hover {{ background: #f9f9f9; }}
        .url-link {{ color: #1976d2; text-decoration: none; }}
        .url-link:hover {{ text-decoration: underline; }}
        .search-box {{ width: 100%; padding: 10px; margin-bottom: 10px; border: 1px solid #ddd; border-radius: 3px; }}
    </style>
</head>
<body>
    <div class="header">
        <h1>ç½‘ç«™å†…å®¹æŠ“å–ç»“æœ</h1>
        <p><strong>ç›®æ ‡ç½‘ç«™:</strong> {self.base_url}</p>
        <p><strong>æŠ“å–æ—¶é—´:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
    </div>
    
    <div class="stats">
        <div class="stat-item">
            <h3>{self.stats['total_discovered']}</h3>
            <p>å‘ç°é¡µé¢</p>
        </div>
        <div class="stat-item">
            <h3>{self.stats['total_crawled']}</h3>
            <p>æˆåŠŸæŠ“å–</p>
        </div>
        <div class="stat-item">
            <h3>{self.stats['failed_crawls']}</h3>
            <p>å¤±è´¥é¡µé¢</p>
        </div>
    </div>
    
    <h2>æŠ“å–çš„é¡µé¢åˆ—è¡¨</h2>
    <input type="text" class="search-box" placeholder="æœç´¢é¡µé¢..." onkeyup="filterUrls(this.value)">
    
    <div class="url-list" id="urlList">
"""
        
        for url, content in self.crawled_content.items():
            title = content.get('title', 'æ— æ ‡é¢˜')
            description = content.get('description', '')[:100] + ('...' if len(content.get('description', '')) > 100 else '')
            
            html_content += f"""
        <div class="url-item" data-url="{url.lower()}" data-title="{title.lower()}">
            <h4><a href="{url}" class="url-link" target="_blank">{title or url}</a></h4>
            <p><strong>URL:</strong> {url}</p>
            {f'<p><strong>æè¿°:</strong> {description}</p>' if description else ''}
            <p>
                <a href="content/{self._url_to_filename(url)}.json">æŸ¥çœ‹JSON</a> | 
                <a href="markdown/{self._url_to_filename(url)}.md">æŸ¥çœ‹Markdown</a>
            </p>
        </div>
"""
        
        html_content += """
    </div>
    
    <script>
        function filterUrls(searchTerm) {
            const items = document.querySelectorAll('.url-item');
            const term = searchTerm.toLowerCase();
            
            items.forEach(item => {
                const url = item.getAttribute('data-url');
                const title = item.getAttribute('data-title');
                
                if (url.includes(term) || title.includes(term)) {
                    item.style.display = 'block';
                } else {
                    item.style.display = 'none';
                }
            });
        }
    </script>
</body>
</html>
"""
        
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write(html_content)


async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    # æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    print("ğŸš€ æ™ºèƒ½ç½‘ç«™çˆ¬è™«å¯åŠ¨ä¸­...")
    print(f"ğŸ“Š å¢å¼ºå¯¼èˆªåŠŸèƒ½: {'âœ… å·²å¯ç”¨' if ENHANCED_NAV_AVAILABLE else 'âŒ æœªå¯ç”¨'}")
    
    if ENHANCED_NAV_AVAILABLE:
        print("ğŸ¯ å¢å¼ºå¯¼èˆªåŠŸèƒ½ç‰¹æ€§:")
        print("   â€¢ 15ç§ä¸“ä¸šå¯¼èˆªé€‰æ‹©å™¨ï¼ˆvs åŸæ¥çš„3ç§ï¼‰")
        print("   â€¢ å®Œæ•´HTMLç»“æ„ä¿ç•™ï¼ˆvs åŸæ¥çš„çº¯æ–‡æœ¬ï¼‰")
        print("   â€¢ å¤šå±‚çº§å¯¼èˆªè¯†åˆ«")
        print("   â€¢ æ™ºèƒ½é“¾æ¥æå–å’Œå»é‡")
        print("   â€¢ å¯¼èˆªç»“æ„åˆ†æå’ŒæŠ¥å‘Š")
    else:
        print("âš ï¸  è¦å¯ç”¨å¢å¼ºå¯¼èˆªåŠŸèƒ½ï¼Œè¯·ç¡®ä¿ enhanced_navigation_extractor.py æ–‡ä»¶å­˜åœ¨")
    
    # ç›®æ ‡ç½‘ç«™URL
    target_url = "https://docs.cursor.com/welcome"
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = WebsiteCrawler(target_url, "cursor_docs_crawl")
    
    try:
        # ç¬¬ä¸€æ­¥ï¼šå‘ç°ç½‘ç«™ç»“æ„
        discovered_urls = await crawler.discover_website_structure(
            max_depth=3,  # æœ€å¤§æ·±åº¦
            max_pages=50  # æœ€å¤§é¡µé¢æ•°
        )
        
        print(f"\nğŸ” å‘ç°çš„é¡µé¢æ•°é‡: {len(discovered_urls)}")
        print("å‰10ä¸ªé¡µé¢:")
        for i, url in enumerate(discovered_urls[:10], 1):
            print(f"  {i:2d}. {url}")
        
        # ç¬¬äºŒæ­¥ï¼šæŠ“å–æ‰€æœ‰å†…å®¹
        results = await crawler.crawl_all_content(
            discovered_urls,
            batch_size=5  # æ‰¹å¤„ç†å¤§å°
        )
        
        print(f"\nğŸ“Š æŠ“å–ç»“æœ:")
        print(f"  âœ… æˆåŠŸæŠ“å–: {results['successful_crawls']} ä¸ªé¡µé¢")
        print(f"  âŒ å¤±è´¥é¡µé¢: {results['failed_crawls']} ä¸ªé¡µé¢")
        print(f"  ğŸ“ è¾“å‡ºç›®å½•: {results['output_directory']}")
        print(f"  ğŸ“Š å¢å¼ºå¯¼èˆª: {'âœ… å·²å¤„ç†' if ENHANCED_NAV_AVAILABLE else 'âŒ æœªå¤„ç†'}")
        
        if ENHANCED_NAV_AVAILABLE:
            print(f"\nğŸ‰ æŠ“å–å®Œæˆ! è¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ä»¥ä¸‹æ–‡ä»¶:")
            print(f"  â€¢ index.html - ä¸»ç´¢å¼•é¡µé¢")
            print(f"  â€¢ enhanced_navigation.json - å¢å¼ºå¯¼èˆªæ•°æ®")
            print(f"  â€¢ navigation_report.html - å¯¼èˆªæ åˆ†ææŠ¥å‘Š")
        else:
            print(f"\nğŸ‰ æŠ“å–å®Œæˆ! è¯·æŸ¥çœ‹è¾“å‡ºç›®å½•ä¸­çš„ index.html æ–‡ä»¶")
        
    except Exception as e:
        logger.error(f"âŒ çˆ¬è™«æ‰§è¡Œå¤±è´¥: {str(e)}")
        raise


if __name__ == "__main__":
    # è¿è¡Œçˆ¬è™«
    asyncio.run(main()) 